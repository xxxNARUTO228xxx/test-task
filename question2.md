# Вопрос 2
Есть таблица с историей расчётов коммерческих предложений (миллионы строк), где встречаются пропуски, дубли, выбросы и не консистентные единицы измерений. Данные должны стать фичами для модели.
Опишите, как вы построите ETL-процесс, если стек: Airflow + ClickHouse + dbt.
Укажите:
- архитектуру витрин (staging - core - mart),
- как будете контролировать качество данных (DQ),
- как будете версионировать и документировать модели витрин,
- как будете обновлять данные (batch / increment / upsert).

# Ответ
## Архитектура витрин (Staging → Core → Mart)
Архитектура построения витрины данных основана на концепции разделения зон ответственности, разделяя процесс преобразования данных на три основных этапа:

- STAGING — зона сырых данных (raw data):
Исходные таблицы из разных источников выгружаются в сыром виде (такими, какими они пришли из исходников).
Здесь хранятся необработанные данные, допускающие наличие пропусков, дублей и некорректных единиц измерения.
- CORE — область преобразованных данных (cleansed and transformed data):
Осуществляется очистка данных: удаление дубликатов, заполнение пропусков, приведение единиц измерения к единому формату.
Применяются правила бизнес-доменов, фильтрация ненужных записей, формирование базовых аналитических сущностей.
- MART — витрина готовых данных (analytical ready data):
Окончательная обработка данных, подготовка аналитики для машинного обучения (фичи для моделей).
Создание агрегированных представлений, рассчитываются ключевые показатели эффективности (KPI), финальная стандартизация атрибутов.

## Контроль качества данных (Data Quality Control)
Контроль качества данных осуществляется на каждом этапе жизненного цикла данных:

1. Проверка валидности данных:
    - Проверка соответствия типов данных (числа, строки, даты).
    - Оценка наличия пропущенных значений, выбросов и экстремальных значений.
2. Контроль целостности данных:
    - Выявление и устранение дублирующих записей.
    - Анализ непротиворечивости данных (например, контроль суммы ценовых позиций коммерческого предложения относительно итогового результата).
3. Обеспечение согласованности данных:
    - Приведение всех показателей к единым единицам измерения (если разные поставщики используют разные единицы).
    - Учет временной шкалы (не допустить смешивания старых и новых расчетов).

Используемые инструменты контроля качества:
1. dbt Data Tests: встроенные возможности инструмента dbt позволяют писать юнит-тесты для данных.
2. Airflow DagRun Sensor: использование датчиков для контроля успешности завершения предыдущих шагов ETL-процесса.

## Версионирование и документация моделей витрин
Версионирование моделей витрин выполняется с использованием инструментов dbt:

- Модули и макросы: модули используются для повторяющихся операций (очистка полей, расчет агрегатов).
- Документация: автоматическое создание документации для каждой модели, содержащей описание схемы данных, формулы расчета, ограничения и рекомендации по применению.
- Автоматическое обновление doc-сайтов: каждый раз при выполнении dbt-моделей генерируется актуальная версия документации.
Дополнительно ведется журнал версий данных с фиксацией дат и авторов изменений.

## Стратегия обновления данных (Batch vs Incremental vs Upsert)
Стратегии обновления данных зависят от объема и частоты поступления новых данных:

Full Batch Update:
- Полная перезагрузка данных из-за больших объемов исторических данных и редких апдейтов.
Используется редко ввиду большого объёма данных.

Incremental Load:
- Загрузка только новых данных с последующим объединением с существующими записями.
Эффективнее, особенно если данные поступают часто небольшими порциями.

Upsert (Update or Insert):
- Обновление существующих записей и вставка новых одновременно.
Подходит, если в данных возможны изменения уже внесенных ранее записей.
Оптимальным решением является сочетание Incremental Load и Upsert, которое минимизирует нагрузку на базу данных и повышает эффективность процесса ETL.

Обычно почти везде подходит Incremental.

## Итоговая архитектура ETL-процесса
ETL Pipeline:

- Источник данных (ClickHouse) → STAGING (загрузка сырого набора данных)
- Очистка и преобразование (AirflowDAG+dbt) → CORE (обработка данных, очистку пропусков, дублирования, приведение к единообразному виду)
- Преобразованные данные → MART (создание готовых для анализа и моделирования структур)
- Применение проверок качества данных (dbt tests, quality checks in Airflow)
- Генерация отчетов и документов для моделей витрин (документация в dbt)
- Регулярное обновление данных согласно выбранной стратегии (incremental)
